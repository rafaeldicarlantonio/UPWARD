{"id": "implicate_001", "query": "How does attention mechanism relate to BERT's contextual understanding?", "category": "implicate_lift", "expected_source_ids": ["doc_transformer_003", "doc_bert_004"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Query requires bridging 'attention mechanism' (doc_003) with 'BERT contextual understanding' (doc_004). Legacy keyword search may miss the connection since neither document explicitly mentions both concepts together. Implicate/graph should bridge through transformer architecture."}
{"id": "implicate_002", "query": "What optimization methods work with backpropagation in neural networks?", "category": "implicate_lift", "expected_source_ids": ["doc_neural_001", "doc_gradient_006"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Requires connecting backpropagation (doc_001) with gradient descent optimization methods (doc_006). These are implicitly related through the training process but may not share direct keyword overlap. Graph should understand the foundational relationship."}
{"id": "implicate_003", "query": "How do semantic embeddings enable retrieval systems?", "category": "implicate_lift", "expected_source_ids": ["doc_embedding_008", "doc_vector_012", "doc_rag_011"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Connects semantic embeddings (doc_008) to vector databases (doc_012) and RAG systems (doc_011). Requires understanding the pipeline: embeddings → vector search → retrieval. Legacy may miss doc_012 and doc_011 without explicit 'embedding' mentions."}
{"id": "implicate_004", "query": "What's the relationship between pretraining and task-specific fine-tuning?", "category": "implicate_lift", "expected_source_ids": ["doc_bert_004", "doc_finetune_009"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Bridges pretrained models like BERT (doc_004) with fine-tuning methodology (doc_009). The connection is through transfer learning workflow but documents may not explicitly cross-reference. Graph should understand this is a standard ML pipeline."}
{"id": "implicate_005", "query": "How does dropout prevent overfitting in deep neural networks?", "category": "implicate_lift", "expected_source_ids": ["doc_regularization_007", "doc_deep_002"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Connects dropout regularization (doc_007) with deep learning architectures (doc_002). While doc_007 mentions dropout, the connection to 'deep neural networks' requires bridging to doc_002. Legacy keyword match may miss this implicit relationship."}
{"id": "implicate_006", "query": "What capabilities emerge when language models scale to billions of parameters?", "category": "implicate_lift", "expected_source_ids": ["doc_llm_010", "doc_scaling_019"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Requires bridging emergent capabilities in LLMs (doc_010) with distributed training infrastructure (doc_019). The query asks about 'scaling' which connects to both emergent abilities and the systems needed to achieve scale. Graph should understand this bidirectional relationship."}
{"id": "implicate_007", "query": "How does few-shot prompting relate to instruction tuning?", "category": "implicate_lift", "expected_source_ids": ["doc_prompt_013", "doc_finetune_009"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Connects prompting techniques (doc_013) with instruction tuning methodology (doc_009). While doc_013 mentions instruction tuning, understanding the broader relationship with fine-tuning requires graph bridging. Legacy may miss the fine-tuning connection."}
{"id": "implicate_008", "query": "What role does human feedback play in aligning language models?", "category": "implicate_lift", "expected_source_ids": ["doc_rlhf_014", "doc_safety_018"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Bridges RLHF methodology (doc_014) with AI safety concerns (doc_018). The query uses 'aligning' which is central to both documents but requires understanding that RLHF is a technique for achieving alignment/safety. Graph should capture this goal-method relationship."}
{"id": "implicate_009", "query": "How do joint embeddings enable vision-language understanding?", "category": "implicate_lift", "expected_source_ids": ["doc_multimodal_016", "doc_embedding_008"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Connects multimodal joint embeddings (doc_016) with fundamental embedding concepts (doc_008). Understanding 'joint embeddings' requires knowledge of what embeddings are in general. Legacy may miss doc_008 since it focuses on word embeddings, not multimodal."}
{"id": "implicate_010", "query": "How does chain-of-thought reasoning enhance agent tool use?", "category": "implicate_lift", "expected_source_ids": ["doc_agents_017", "doc_prompt_013"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Bridges LLM agents (doc_017) with chain-of-thought prompting (doc_013). While doc_017 mentions reasoning, the specific connection to 'chain-of-thought' technique requires understanding that CoT is a prompting method that enhances reasoning. Graph should link these concepts."}
{"id": "implicate_011", "query": "What quantization techniques make large models deployable?", "category": "implicate_lift", "expected_source_ids": ["doc_efficiency_020", "doc_llm_010"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Connects model compression/quantization (doc_020) with large language models (doc_010). The query implies: LLMs are large → need compression → quantization enables deployment. Legacy may not bridge 'large models' to doc_010 without explicit mention of deployment."}
{"id": "implicate_012", "query": "How do we evaluate reasoning capabilities in large language models?", "category": "implicate_lift", "expected_source_ids": ["doc_eval_015", "doc_llm_010"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Bridges evaluation methods (doc_015) with LLM capabilities (doc_010). Query asks about evaluating 'reasoning' which is an emergent capability mentioned in doc_010, but requires doc_015 for evaluation methodology. Graph should understand evaluation is for LLM capabilities."}
{"id": "implicate_013", "query": "How does adversarial training improve model robustness?", "category": "implicate_lift", "expected_source_ids": ["doc_safety_018", "doc_regularization_007"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Connects adversarial training (doc_018) with general robustness/regularization concepts (doc_007). While doc_018 mentions adversarial training, understanding it as a form of regularization requires bridging to doc_007. Graph should understand adversarial training as a robustness technique."}
{"id": "implicate_014", "query": "How do autoregressive transformers like GPT generate coherent text?", "category": "implicate_lift", "expected_source_ids": ["doc_gpt_005", "doc_transformer_003"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Bridges GPT architecture (doc_005) with foundational transformer concepts (doc_003). Understanding 'autoregressive transformers' requires knowledge that GPT is built on transformer architecture. Legacy may miss doc_003 since GPT doc doesn't explicitly define transformers."}
{"id": "implicate_015", "query": "What distributed training strategies enable efficient model parallelism?", "category": "implicate_lift", "expected_source_ids": ["doc_scaling_019", "doc_efficiency_020"], "expected_in_top_k": 8, "max_latency_ms": 500, "legacy_should_miss": true, "rationale": "Connects distributed training (doc_019) with efficiency goals (doc_020). The query implies: parallelism is a method → for efficiency. While doc_019 covers parallelism, understanding it as an efficiency technique requires bridging to doc_020. Graph should link these optimization concerns."}
